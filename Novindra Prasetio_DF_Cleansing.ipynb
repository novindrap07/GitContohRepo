{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from datetime import datetime, date\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import missingno as msno "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File b'docker_retail_masking (1).csv' does not exist: b'docker_retail_masking (1).csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-7072b0dfa0fb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Read dataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdf1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"docker_retail_masking (1).csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mdf2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"project_masking.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mdf3\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"so_retail_new (1).csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mdf1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    700\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[0;32m    701\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 702\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    703\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    704\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    427\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    428\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 429\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    430\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    431\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    894\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 895\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    896\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    897\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1120\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'c'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1121\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'c'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1122\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1123\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1124\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'python'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1851\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'usecols'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1852\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1853\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1854\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1855\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] File b'docker_retail_masking (1).csv' does not exist: b'docker_retail_masking (1).csv'"
     ]
    }
   ],
   "source": [
    "# Read dataset\n",
    "df1 = pd.read_csv(\"docker_retail_masking (1).csv\")\n",
    "df2 = pd.read_csv(\"project_masking.csv\")\n",
    "df3 = pd.read_csv(\"so_retail_new (1).csv\") \n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.rename(columns={'ID':'id'}, inplace=True)\n",
    "df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Docker Retail = Kolom Supir :\n",
    "## Lower Case\n",
    "df1[\"Supir\"] = df1[\"Supir\"].str.lower()\n",
    "\n",
    "## Fill Null = -\n",
    "df1[\"Supir\"] = df1[\"Supir\"].fillna(\"-\")\n",
    "df1[[\"Supir\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Get 'kode' from nomor_retail & docket_number_final\n",
    "get = df1['nomor_retail'].str.split(\"/\", n = 2, expand=True)\n",
    "df1['kode_unit'] = get[1]\n",
    "\n",
    "it = df1['docket_number_final'].str.split(\"/\", n = 2, expand=True )\n",
    "df1['kode_organisasi'] = it[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### List Kode Divisi\n",
    "kode_div = ['DV1', 'DV2', 'DV3', 'DOFF']\n",
    "df3['kode_div'] = np.where(df3[\"kode_div\"].str.contains('|'.join(kode_div)),df3['kode_div'], np.nan)\n",
    "df3['kode_div'].fillna('-', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### List Nama Divisi\n",
    "df2['plant_nama'] = df2['plant_nama'].str.lower()\n",
    "nama_div = ['divisi manufaktur', 'divisi precast', 'divisi property', 'divisi readymix']\n",
    "df2['nama_div'] = np.where(df2[\"plant_nama\"].str.contains('|'.join(nama_div)), df2['plant_nama'], np.nan)\n",
    "df2['nama_div'].fillna('-', inplace=True)\n",
    "df2['nama_div']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plant_nama disesuaikan dengan objektif kata 'Plant'\n",
    "plant_nama = ['plant bandara', 'plant bandung barat',\n",
    "       'plant bandung timur', 'plant beam surabaya', 'plant bsd',\n",
    "       'plant cakung', 'plant casablanca', 'plant cibitung',\n",
    "       'plant cikarang', 'plant cikupa', 'plant cilacap', 'plant dadap',\n",
    "       'plant dawuan', 'plant delta mas cikarang', 'plant depo lrt',\n",
    "       'plant icon', 'plant kebon jeruk', 'plant kemayoran',\n",
    "       'plant lenteng agung', 'plant mauk',\n",
    "       'plant pile & slab jakarta xxx', 'plant precast cibitung',\n",
    "       'plant precast cikarang', 'plant precast cilacap',\n",
    "       'plant precast karawang', 'plant precast koala tanjung',\n",
    "       'plant precast osowilangun', 'plant pulo gadung',\n",
    "       'plant readymix cibubur', 'plant readymix cilacap',\n",
    "       'plant readymix osowilangun', 'plant readymix precast cilacap',\n",
    "       'plant readymix tambak sumur', 'plant readymix transpark bekasi',\n",
    "       'plant readymix trembesi - kemayoran', 'plant sentul',\n",
    "       'plant site mix kertajati cirebon',\n",
    "       'plant site mix readymix palimanan', 'plant taman anggrek ii',\n",
    "       'plant tanah abang']\n",
    "df2['plant_nama'] = np.where(df2[\"plant_nama\"].str.contains('|'.join(plant_nama)), df2['plant_nama'], np.nan)\n",
    "df2['plant_nama'].fillna('-', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# SO_Retail\n",
    "## Mutu dan Spesifikasi lower case\n",
    "df3['Spesifikasi'] = df3['Spesifikasi'].str.lower()\n",
    "df3['Mutu'] = df3['Mutu'].str.lower()\n",
    "\n",
    "## Buat kolom baru dari beberapa values kolom Spesifikasi\n",
    "### Mutu\n",
    "Moet = df3['Mutu'].str.split(n=1, expand=True)\n",
    "df3['Mutu'] = Moet[0]\n",
    "\n",
    "### Spesifikasi\n",
    "df3['Spesifikasi_new'] = np.where(df3[\"Mutu\"].str.contains('fa'),df3['Mutu'], \n",
    "                                  np.where(df3['Mutu'].str.contains('nfa'), df3['Mutu'],''))\n",
    "### Spek_Tambahan\n",
    "df3['Spesifikasi_tambahan'] = np.where(df3[\"Spesifikasi\"].str.contains('spesial'),df3['Spesifikasi'], \n",
    "                                  np.where(df3['Spesifikasi'].str.contains('fast'), df3['Spesifikasi'],\n",
    "                                          np.where(df3['Spesifikasi'].str.contains('track'), df3['Spesifikasi'],\n",
    "                                                  np.where(df3['Spesifikasi'].str.contains('fastrack'), df3['Spesifikasi'],\n",
    "                                                          np.where(df3['Spesifikasi'].str.contains('ctb'), df3['Spesifikasi'],\n",
    "                                                                  np.where(df3['Spesifikasi'].str.contains('porous'), df3['Spesifikasi'],''))))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Spesifikasi_new split FA dan NFA\n",
    "new = df3['Spesifikasi_new'].str.split(' ', n=1, expand=True)\n",
    "df3['Spesifikasi_new'] = new[1]\n",
    "df3['Spesifikasi_new'] = df3['Spesifikasi_new'].str.upper()\n",
    "df3['Spesifikasi_new'] = df3['Spesifikasi_new'].fillna('-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Spesifikasi_tambahan\n",
    "#### Replace beberapa values sesuai dengan dictionary\n",
    "##### 'spesial 1 hari'\n",
    "df3['Spesifikasi_tambahan'].replace(['fast track 1 hari'], \n",
    "                                    'spesial 1 hari', inplace=True)\n",
    "\n",
    "##### 'spesial 2 hari'\n",
    "df3['Spesifikasi_tambahan'].replace(['k-600 nfa spesial kering 2 hari'], \n",
    "                                    'spesial 2 hari', inplace=True)\n",
    "\n",
    "##### 'spesial 3 hari'\n",
    "df3['Spesifikasi_tambahan'].replace(['k-350 nfa (fast track 3 hari)', 'fast track 3 hari',\n",
    "                                     'fs-45 nfa spesial 3 hari', 'k-300 nfa fastrack 3 hari',\n",
    "                                     'fast track 3 hari ', 'k-400 nfa fast track 3 hari',\n",
    "                                     'k-350 nfa spesial 3 hari', 'class p-1 nfa spesial 3hari',\n",
    "                                     'k 300 nfa spesial 3 hari', 'class p-1 nfa spesial 3 hari',\n",
    "                                     'k-300 fa fast track 3 hari', ' 3 hari track fasttsafk-400 nfa',\n",
    "                                     'fastrack 3 hari ', 'fs-45 fa fastrack 3 hari',\n",
    "                                     'fc-45 nfa fastrack 3 hari', 'spesial 3 hari 100%',\n",
    "                                     'fast trak 3 hari', 'k-500 nfa spesial 3 hari',\n",
    "                                     'fs-45 nfa (fastrack 3 hari)', 'fs-45 nfa (fastrack 3 hari )', \n",
    "                                     'spesial 3 hari fs-45 nfa', \n",
    "                                     'fs-45 nfa  (fastrack 3 hari )', 'spesial 3 harifs-45 nfa',\n",
    "                                     'fs-45 nfa spesial 3 hari ', 'fs-45 nfa (fast track 3 hari )',\n",
    "                                     '\\tfastrack 3 hari', 'fs-45 nfa  fastrack 3 hari',\n",
    "                                     ' fastrack 3 hari', 'spesial 3 hari ',\n",
    "                                     '(fastrack 3 hari)', ' (fastrack 3 hari)', \n",
    "                                     'fastrack  3 hari', 'fastrack 3 hari\\t', \n",
    "                                     'fastrack 3 hari','fc-40 nfa spesial 3 hari',\n",
    "                                     'k-350 nfa fast track 3 hari scr '], \n",
    "                                    'spesial 3 hari', inplace=True)\n",
    "\n",
    "##### 'spesial 7 hari'\n",
    "df3['Spesifikasi_tambahan'].replace(['fast track 7 hari',\n",
    "                                     'k-350 nfa spesial 7 hari',\n",
    "                                     'spesial 7 hari ',\n",
    "                                     'fc-30 nfa spesial 7 hari'], \n",
    "                                    'spesial 7 hari', inplace=True)\n",
    "\n",
    "##### 'spesial 10 hari'\n",
    "df3['Spesifikasi_tambahan'].replace(['k-225 nfa spesial 10 hari'], \n",
    "                                    'spesial 10 hari', inplace=True)\n",
    "\n",
    "##### 'spesial 14 hari'\n",
    "df3['Spesifikasi_tambahan'].replace(['k-225 nfa fast track 14 hari',\n",
    "                                      'k-350 nfa spesial 14 hari',\n",
    "                                     'fast track 14 hari',\n",
    "                                     'k-300 nfa spesial 14 hari',\n",
    "                                     'k-300 fa spesial 14 hari'], \n",
    "                                    'spesial 14 hari', inplace=True)\n",
    "##### 'spesial 28 hari'\n",
    "df3['Spesifikasi_tambahan'].replace(['fs-45 nfa (spesial 28 hari)', \n",
    "                                     'fs-45 nfa spesial 28 hari'], \n",
    "                                    'spesial 28 hari', inplace=True)\n",
    "\n",
    "##### 'ctb'\n",
    "df3['Spesifikasi_tambahan'].replace(['k-225 nfa spesial 10 hari', 'ctb k125',\n",
    "                                    'ctb k125', 'ctb-c-0 fa k125',\n",
    "                                    'k-150 nfa ctb', 'k-300 fa ctb', \n",
    "                                    'ctb-c-0 fa k 150','ctb-c-0 fa k 125'], \n",
    "                                    'ctb', inplace=True)\n",
    "\n",
    "##### 'spasi'\n",
    "df3['Spesifikasi_tambahan'].replace([''], \n",
    "                                    '-', inplace=True)\n",
    "\n",
    "##### fillna('-')\n",
    "df3['Spesifikasi_tambahan'].fillna('-', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### is_web == retail_method\n",
    "df3['is_website'] = np.where(df3[\"retail_method\"].str.contains('manual'),'false','true') \n",
    "df3[['retail_method','is_website']]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Alamat_proyek\n",
    "df3['Alamat_proyek'] = df3['street'] +'- '+ df3['street2']\n",
    "df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### change start_time type from object to datetime/timestamp\n",
    "df1['start_time'] = pd.to_datetime(df1.start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Fill null in realisasi_volume_terima with mean/average\n",
    "df1['realisasi_volume_terima'].fillna(df1['realisasi_volume_terima'].mean(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## inner join df1, df2 & df3\n",
    "result = pd.concat([df1, df2, df3], keys=['df1', 'df2', 'df3'], axis=1, join='inner')\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Drop some columns\n",
    "result.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kode_unit = ['U1-2', 'UR', 'U2-1', 'U1-1', 'U2-3', \n",
    "#              'U1-4', 'U1-3', 'U1-5','U2-2', 'U2-4', \n",
    "#              'U3-1', 'UT', 'False', 'RMC']\n",
    "\n",
    "# kode_org = ['CSA', 'CBR', 'DWN', 'DMS', 'KJI', 'CKG', 'BSD', 'STL', 'TMA',\n",
    "#        'DDP', 'PST', 'LTA', 'TSR', 'BTR', 'CKP', 'TNA', 'BBT', 'BKTR',\n",
    "#        'GLE', 'MAK', 'CBT', 'CLP', 'KBJ', 'PLMN', 'MON', 'OW',\n",
    "#        'TNA2', 'KLS', 'LTAO', 'KPO']\n",
    "\n",
    "#### Kode_Unit\n",
    "# df3['kode_unit'] = np.where(df3[\"kode_unit\"].str.contains('|'.join(kode_unit)), df3['kode_unit'], \n",
    "#                             np.where(df3[\"kode_unit\"].str.contains('|'.join(kode_org)), np.nan, np.nan))\n",
    "# df3['kode_unit'].fillna('-')\n",
    "\n",
    "# #### Kode_Organisasi\n",
    "# df3['kode_organisasi'] = np.where(df3[\"kode_organisasi\"].str.contains('|'.join(kode_org)), df3['kode_organisasi'], \n",
    "#                             np.where(df3[\"kode_organisasi\"].str.contains('|'.join(kode_div)), np.nan, \n",
    "#                                      np.where(df3[\"kode_organisasi\"].str.contains('|'.join(kode_unit)), np.nan, np.nan)))\n",
    "# df3['kode_organisasi'].fillna('-')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
